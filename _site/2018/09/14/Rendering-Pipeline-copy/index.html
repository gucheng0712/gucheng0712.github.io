<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="stylesheet" type="text/css" href="/assets/css/materialize.css"> <link rel="stylesheet" type="text/css" href="/assets/main.css"> <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"> <link rel="stylesheet" href="https://cdn.rawgit.com/konpa/devicon/df6431e323547add1b4cf45992913f15286456d3/devicon.min.css"> <title>Suzuka Site</title> </head> <body> <div class="container"> <div class="col s10"> <div class="row" style="padding:10px;"> <a id="home" class="waves-effect waves-light btn white-text darken-2 grey z-depth-4" href="/#blog"><i class="material-icons">arrow_back</i></a> </div> <h3 class="post-title" itemprop="name headline" style="text-align:center">Rendering Pipeline</h3> <p class="post-meta"> <time datetime="2018-09-13T18:00:05-07:00" itemprop="datePublished"> Sep 14, 2018 </time> </p> <div class="post-content" itemprop="articleBody"> <p>The pipeline of Graphic Rendering includes 3 stages: Application Stage –&gt; Geometry Stage –&gt; Rasterization Stage.</p> <p class="center"><img src="/assets/images/PostImages/RenderingPipeline.png" alt="dot" height="50%" width="50%"/></p> <hr/> <h4 id="1-application-stage">1. Application Stage</h4> <p>The start point of the rendering pipeline is CPU. In the Application Stage, the most important output is the geometry infomation which need to be rendered, called <strong><em>Rendering Primitives.</em></strong></p> <blockquote> <p>In this Stage, the developer has 3 Tasks:</p> </blockquote> <ol> <li> <p>prepare the environment and camera’s position. how many models are used in the environment, and which kinds of light sources are used in the scene.</p> </li> <li> <p>In order to improve the performance of rendering, need to do the work of <strong><em>Culling</em></strong>, to get rid of unnecessary rendering process.</p> </li> <li> <p>Need to set up the rendering state of each model, such as the materials , textures, shaders.</p> </li> <li> <p>CPU will call <strong><em>DrawCall()</em></strong> to make GPU draw stuff on screen.</p> </li> </ol> <blockquote> <p>For example.</p> </blockquote> <p class="center"><img src="/assets/images/PostImages/applicationStage.png" alt="dot" height="50%" width="50%"/></p> <hr/> <h4 id="2-geometry-stage">2. Geometry Stage</h4> <p>In Geometry Stage, the GPU will do the vertex operation through the received vertex data from the CPU.</p> <p class="center"><img src="/assets/images/PostImages/GeometryStage.png" alt="dot" height="70%" width="70%"/></p> <blockquote> <p>The Process:</p> </blockquote> <ol> <li> <p>Vertex Shader: programmable , it is usually used to implement vertex space transformation, vertex shading and other functions.</p> </li> <li> <p>Tesselation Shader: An optional shader for subdividing primitives (primitives are the basic units that make up an image, such as points, lines, surfaces, etc.).</p> </li> <li> <p>Geometry Shader: An optional shader for polygon shading operations.</p> </li> <li> <p>Clipping: this phase is configurable. The goal of this phase is to trim away the vertices that are not in the camera’s field of view and to remove the facets of certain trig elements.</p> </li> <li> <p>Screen Mapping: This stage is not configurable and is not programmatic; it is responsible for converting the coordinates of each pixel to the screen coordinate system.</p> </li> </ol> <blockquote> <p>Meaning of_MVP (Model View Projection):</p> </blockquote> <p>The <strong><em>vertices</em></strong> of a model are stored in the <strong><em>Object Space</em></strong>. The <strong><em>position</em></strong> and the <strong><em>orientation</em></strong> of each model are stored in world space.</p> <p>Before an object can be rendered, its vertices must be transformed into the <strong><em>Camera Space</em></strong> (also called View Space).</p> <p>It is possible to transform vertices from object space directly into camera space by concatenating the matrices representing the transformations from object space to world space and from world space to camera space. The product of these transformations is called the <strong><em>Model-View Trasnformation</em></strong>. Once a model’s vertices have been transformed into camera space, they will also do a projection transformation to apply <strong><em>Perspective</em></strong> (the geometry becomes smaller as the distance from the camera increases).</p> <hr/> <h4 id="3-rasterization-stage">3. Rasterization Stage</h4> <p>In this stage, will use the data passed in the previous stage(Geometry stage) to produce pixels on the screen and render the final image.</p> <p class="center"><img src="/assets/images/PostImages/RasterizationStage.png" alt="dot" height="70%" width="70%"/></p> <blockquote> <p>The Process:</p> </blockquote> <p>1) <strong><em>Triangle Setup</em></strong>: Differences in triangular surfaces and other relevant data are calculated. Fixed hardware operations to complete.</p> <p>2) <strong><em>Triangle Traversal</em></strong>: This stage checks whether each pixel is inside a triangle, and fragment is generated at this stage. Interpolation is usually used.</p> <p>3) <strong><em>Fragment Shader</em></strong>: Fully programmable. Use interpolated shading data as input and output one or more color values for the next phase. At this stage, GPU execution can be controlled by programming. The most commonly used technology in this stage is texture technology.</p> <p>4) <strong><em>Per-Fragment Operation</em></strong>: The Color value for each pixel is stored in the Color Buffer and merged at this stage. This stage is not completely programmable, but can be configured to produce different effects. In detail includes:</p> <blockquote> <p>a) <strong><em>The pixel ownership</em></strong> (the only one cannot be disabled): it simply determines whether a fragment lies in the region of the viewport that is currently visible on the display.</p> </blockquote> <blockquote> <p>b) <strong><em>Scissor Test</em></strong>: An application may specify a rectangle in the viewport, called the scissor rectangle, to which rendering should be restricted. Any fragments falling outside the scissor rectangle are discarded.</p> </blockquote> <blockquote> <p>c) <strong><em>Alpha Test</em></strong>: When the final color of a fragment is calculated, an application may also calculate an alpha value that usually represents the degree of transparency associated with the fragment. The alpha test compares the final alpha value of a fragment to a constant value that is preset by the application. The application can specifies what relationship between the two values (such as less than, greater than, or equal to) causes the test to pass. If the relationship is not satisfied, then the fragment is discarded.</p> </blockquote> <blockquote> <p>d) <strong><em>Stencil Test</em></strong>: The sten- cil test reads the value stored in the stencil buffer at a fragment’s location and compares it to a value previously specified by the application. The stencil test passes only if a specific relationship is satisfied (e.g., the stencil value is equal to a particular value); otherwise, the stencil test fails, and the fragment is discarded.</p> </blockquote> <blockquote> <p>e) <strong><em>Depth Test</em></strong>: The depth Test compares the final depth associated with a fragment to the value currently residing in the depth buffer.</p> </blockquote> <blockquote> <p>f) <strong><em>Blending</em></strong>: Once the pixel ownership test, scissor test, alpha test, stencil test, and depth test have all passed, a fragment’s final color is blended into the <strong><em>image buffer</em></strong>. The blending operation calculates a new color by combining the fragment’s final color and the color already stored in the image buffer at the fragment’s location. The fragment’s alpha value and the alpha value stored in the image buffer may also be used to determine the color that ultimately appears in the viewport. The blending operation may be configured to simply replace the previous color in the image buffer, or it may produce special visual effects such as transparency.</p> </blockquote> <hr/> <blockquote> <p><strong>End –Cheng Gu</strong></p> </blockquote> </div> </div> </div> <script type="text/javascript" src="/assets/js/jquery-3.2.1.min.js"></script> <script type="text/javascript" src="/assets/js/materialize.min.js"></script> <script src="/assets/js/init.js"></script> </body> <footer class="page-footer"> <div class="footer-copyright" style="padding-left: 30px"> <i class="material-icons" style="padding-right:5px">copyright</i> <p>2018 Copyright Cheng Gu</p> </div> </footer> </html>