<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1 maximum-scale=1; minimum-scale=1; user-scalable=no;"> <meta name="author" content="ChengGu"> <meta name="baseurl" content=""> <title> Suzuka Site|Convolutional Neuron Network (ML) </title> <!-- favicon --> <link rel="shortcut icon" href="/static/assets/img/favicon.ico"> <!-- Main CSS --> <link href="/static/assets/app-20180125.min.css" rel="stylesheet"> <link href="/static/css/custom.css" rel="stylesheet"> <!-- Main Scripts --> <script src="/static/assets/app-20180125.min.js"></script> <script src="/static/assets/blog-20180125.min.js"></script> <!-- Google AdSense --> <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> <script> (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: "ca-pub-6196184668650108", enable_page_level_ads: true }); </script> </head> <body id="page-top" class="landing-page"> <div class="search-tool" style="position: fixed; top: 0px ; bottom: 0px; left: 0px; right: 0px; opacity: 0.95; background-color: #111111; z-index: 9999; display: none;"> <input type="text" class="form-control search-content" id="search-content" style="position: fixed; top: 60px" placeholder="Search Blog"> <div style="position: fixed; top: 16px; right: 16px; z-index: 9999;"> <img src=" /static/assets/img/search/cb-close.png " id="close-btn" /> </div> </div> <div style="position: fixed; right: 16px; bottom: 20px; z-index: 9999;"> <img src=" /static/assets/img/search/cb-search.png " id="search-btn" title="Double click Ctrl" /> </div> <div class="navbar-wrapper"> <nav class="navbar navbar-default navbar-fixed-top" role="navigation"> <div class="container"> <div class="navbar-header page-scroll"> <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href=" / ">Suzuka Site</a> </div> <div id="navbar" class="navbar-collapse collapse"> <ul class="nav navbar-nav navbar-right"> <li><a class="page-scroll" href=" blog/ "></a></li> <li> <a class="page-scroll" href="/blog/">Blog</a></li> <li> <a class="page-scroll" href="/unity/">Unity</a></li> <li> <a class="page-scroll" href="/ai/">AI</a></li> <li> <a class="page-scroll" href="/cg/">CG</a></li> <li> <a class="page-scroll" href="/math/">Math</a></li> <li> <a class="page-scroll" href="/data structure/">Data Structure</a></li> <li> <a class="page-scroll" href="/csharp/">C#</a></li> <li> <a class="page-scroll" href="/python/">Python</a></li> <li> <a class="page-scroll" href="/javascript/">JavaScript</a></li> <li> <a class="page-scroll" href="/game dev/">Game Dev</a></li> </ul> </div> </div> </nav> </div> <!--<link rel='stylesheet' href='https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css'>--> <!--<link rel="stylesheet" href="static/css/carousel_style.css">--> <!--<script src="static/js/carousel.js"></script>--> <div id="carouselHacked" class="carousel slide carousel-fade" data-ride="carousel"> <div class="carousel-inner" role="listbox"> <div class="item active"> <!-- Set background for slide in css --> <div class="header-back" style="background-image: url(/static/assets/img/landing/header_1.jpg);" title="first banner image"> </div> </div> <div class="item"> <div class="header-back" style="background-image: url(/static/assets/img/landing/header_2.jpg);" title=""> </div> </div> <div class="item"> <div class="header-back" style="background-image: url(/static/assets/img/landing/header_3.jpg);" title="third banner image"> </div> </div> <div class="item"> <div class="header-back" style="background-image: url(/static/assets/img/landing/header_4.jpg);" title="fourth banner image"> </div> </div> <div class="item"> <div class="header-back" style="background-image: url(/static/assets/img/landing/header_5.jpg);" title="fifth banner image"> </div> </div> <div class="item"> <div class="header-back" style="background-image: url(/static/assets/img/landing/header_6.jpg);" title="sixth banner image"> </div> </div> </div> <!-- Controls --> <a class="left carousel-control" href="#carouselHacked" role="button" data-slide="prev"> <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span> <span class="sr-only">Previous</span> </a> <a class="right carousel-control" href="#carouselHacked" role="button" data-slide="next"> <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span> <span class="sr-only">Next</span> </a> </div> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true } }); </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <div class="wrapper wrapper-content animated fadeInRight article"> <div class="row"> <div class="col-lg-10 col-lg-offset-1"> <div class="ibox"> <div class="ibox-content"> <div class="pull-right"> <a class="btn btn-white btn-xs" href="/python">Python</a> <a class="btn btn-white btn-xs" href="/ai">Ai</a> </div> <div class="text-center article-title"> <span class="text-muted"><i class="fa fa-clock-o"></i> 16 Apr 2019</span> <h1> Convolutional Neuron Network (ML) </h1> </div> <div class="ibox-postpadding"> <h1 id="what-is-convolutional-neuron-network"><strong>What is Convolutional Neuron Network?</strong></h1> <p><code class="highlighter-rouge">Convolutional Neural Networks (CNNs)</code> are also known as <code class="highlighter-rouge">convnets</code>. These types of deep learning models are widely used in <code class="highlighter-rouge">computer vision</code> applications.</p> <p>In mathematics <code class="highlighter-rouge">convolution</code> is an operation on two functions to produce a third function that expresses how the shape of one is modified by the other.</p> <p>In <code class="highlighter-rouge">image processing convolution</code> involves a kernel (small matrix) that is used for operations like blurring, sharpening, edge detection, etc. <a href="https://gucheng0712.github.io/cg/ai/math/unity/2019/04/03/Convolution-in-Image-Processing-(CG&amp;ML).html">(Details about convolution in Image Processing)</a></p> <p align="center"> <img src="/static/assets/img/blog/cnnbg.jpg" width="50%" /></p> <hr /> <h1 id="densely-connected-layer-vs-concolution-layer"><strong>Densely Connected Layer vs. Concolution Layer</strong></h1> <p><code class="highlighter-rouge">Dense layers</code> learn <code class="highlighter-rouge">global</code> patterns in their inputs. For example, densely connected deep learning model trained on the handwritten digit dataset learns patterns on 28 x 28 pixels. This can result it prediction errors for similar inputs, as shown below image.</p> <p align="center"> <img src="/static/assets/img/blog/denselayer.jpg" width="80%" /></p> <p>By contrast, from the below image, we can see <code class="highlighter-rouge">convolution layers</code> learn <code class="highlighter-rouge">local</code> patterns found in small windows (the dimension of convolution kernel). For the handwritten digits dataset, using this technique results in prediction accuracy above 99%.</p> <p align="center"> <img src="/static/assets/img/blog/clayer.jpg" width="40%" /></p> <hr /> <h1 id="how-does-cnn-works"><strong>How does CNN Works?</strong></h1> <hr /> <h2 id="the-convolution-operation"><strong>The convolution operation</strong></h2> <p>The patterns learned by CNNs are <code class="highlighter-rouge">translation invariant</code>. After learning a certain pattern in one part of an input image, a convnet can recognize it <code class="highlighter-rouge">anywhere</code>. This makes convnets data efficient when processing images.</p> <p>CNNs can learn <code class="highlighter-rouge">spatial hierarchies</code> of patterns. A <code class="highlighter-rouge">first</code> convolution layer will learn <code class="highlighter-rouge">small</code> local patterns such as edges, a <code class="highlighter-rouge">second</code> convolution layer will learn <code class="highlighter-rouge">larger</code> patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts. The below Image details how this process works</p> <p align="center"> <img src="/static/assets/img/blog/cnn.jpg" width="60%" /><br /> <i>The visual world forms a spatial hierarchy of visual modules: <br />hyperlocal edges combine into local objects such as eyes or ears, <br />which combine into high-level concepts such as “cat.”</i> </p> <p>Convolutions operate on 3D tensor, called <code class="highlighter-rouge">feature maps</code>, with <code class="highlighter-rouge">two spatial axis</code> (height and width) as well as <code class="highlighter-rouge">a depth axis</code>(also called channels axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray).</p> <p>Then the convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an <code class="highlighter-rouge">output feature map</code> (aka response map).</p> <p>In the MNIST example, the first convolution layer takes a feature map of size <code class="highlighter-rouge">(28, 28, 1)</code> and outputs a feature map of size<code class="highlighter-rouge"> (26, 26, 32)</code>. It computes <code class="highlighter-rouge">32 filters</code> over its input. Each of these <code class="highlighter-rouge">32</code> output channels contains a<code class="highlighter-rouge"> 26 × 26</code> grid of values, which is a response map of the filter over the input, indicating the response of that filter pattern at different locations in the input. (shown as below image)</p> <p align="center"> <img src="/static/assets/img/blog/filteringcnn.jpg" width="80%" /></p> <p>Convolution works by <code class="highlighter-rouge">sliding windows</code> of size 3 × 3 on the 3D input feature map, extracting 3D patches. Each such 3D patch is then <code class="highlighter-rouge">transformed into a 1D vector of shape</code>. All of these vectors are then spatially reassembled into a 3D output feature map.(shown as below image)</p> <p align="center"> <img src="/static/assets/img/blog/hccnw.jpg" width="60%" /></p> <blockquote> <p><code class="highlighter-rouge">Note</code> that the output width and height may differ from the input width and height. They may differ for two reasons:</p> <ol> <li><code class="highlighter-rouge">Border effects</code>, which can be countered by padding the input feature map</li> <li>The use of <code class="highlighter-rouge">strides</code>, which I’ll represents how many steps for each filtering.</li> </ol> </blockquote> <hr /> <h3 id="the-border-effects-and-padding-in-convolution-operation"><strong>The Border Effects and Padding in Convolution Operation</strong></h3> <p>One factor that can influence output size is Border effects. Convolution typically results in an output feature map that is 2 units smaller in width and height. For example, given a 5 × 5 input feature map above, there are 9 tiles overlapping tiles that can fit inside the area, resulting in output feature map that is 3 × 3 tiles. Similarly, starting with a input feature map of 28 × 28, the output feature map will be 26 × 26 after the first convolution layer.(see figure below)</p> <p align="center"> <img src="/static/assets/img/blog/bordereffects.jpg" width="60%" /><br /> <i>Valid locations of 3 × 3 patches in a 5 × 5 input feature map</i> </p> <p>In order to get an output feature map with the same spatial dimensions as the input, using <code class="highlighter-rouge">padding</code> is a solution for this. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile. For a 3 × 3 window, you add one column on the right, one column on the left, one row at the top, and one row at the bottom. For a 5 × 5 window, you add two rows (see figure below).</p> <p align="center"> <img src="/static/assets/img/blog/padding.jpg" width="60%" /><br /> <i>Padding a 5 × 5 input in order to be able to extract 25 3 × 3 patches</i> </p> <p>In <code class="highlighter-rouge">Conv2D</code> layers, padding is configurable via the padding argument, which takes two values: <code class="highlighter-rouge">"valid"</code>, which means no padding (only valid window locations will be used); and <code class="highlighter-rouge">"same"</code>, which means pad in such a way as to have an output with the same width and height as the input. The padding argument defaults to <code class="highlighter-rouge">"valid"</code>.</p> <hr /> <h3 id="convolution-strides"><strong>Convolution Strides</strong></h3> <p>Another factor is the notion of <code class="highlighter-rouge">strides</code>. The description of convolution so far has assumed that the center tiles of the convolution windows are all <code class="highlighter-rouge">contiguous</code>. But the <code class="highlighter-rouge">distance</code> between two successive windows is a parameter of the convolution, called its <code class="highlighter-rouge">stride</code>, which <code class="highlighter-rouge">defaults to 1</code>. It’s possible to have strided convolutions: convolutions with a stride higher than 1. In below image, you can see the patches extracted by a <code class="highlighter-rouge">3 × 3</code> convolution with <code class="highlighter-rouge">stride 2</code> on a <code class="highlighter-rouge">5 × 5</code> input (without padding).</p> <p align="center"> <img src="/static/assets/img/blog/strides.jpg" width="70%" /><br /> <i>3 × 3 convolution patches with 2 × 2 strides</i> </p> <blockquote> <p><code class="highlighter-rouge">Note</code>: Using stride 2 means the width and height of the feature map are downsampled by a factor of 2 (in addition to any changes induced by border effects). Strided convolutions are rarely used in practice, although they can come in handy for some types of models; it’s good to be familiar with the concept. To downsample feature maps, instead of strides, we tend to use the <code class="highlighter-rouge">max-pooling</code> operation, which you saw in action in the first convnet example. Let’s look at it in more depth.</p> </blockquote> <hr /> <h2 id="the-max-pooling-operation"><strong>The Max-Pooling Operation</strong></h2> <p><code class="highlighter-rouge">Max pooling layers</code> are similar to convolutions with a <code class="highlighter-rouge">2 x 2</code> windows and <code class="highlighter-rouge">stride of 2</code>, resulting in output feature maps that are <code class="highlighter-rouge">half</code> of input width and height.</p> <p>For instance, before the first MaxPooling2D layers, the feature map is <code class="highlighter-rouge">26 × 26</code>, but the max-pooling operation halves it to <code class="highlighter-rouge">13 × 13</code>. That’s the role of max pooling: to aggressively downsample feature maps, much like strided convolutions.</p> <p align="center"> <img src="/static/assets/img/blog/MaxPooling.png" width="70%" /><br /> </p> <hr /> <h2 id="mnist-digits-code-example"><strong>MNIST Digits Code Example</strong></h2> <blockquote> <p>The following lines of code show that what a basic convnet looks like. It’s a stack of Conv2D and MaxPooling2D layers.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

</code></pre></div></div> <blockquote> <p>Importantly, a convnet takes as input tensors of shape <code class="highlighter-rouge">(image_height, image_width, image_channels)</code> (not including the batch dimension). In this case, we’ll configure the convnet to process inputs of size <code class="highlighter-rouge">(28, 28, 1)</code>, which is the format of MNIST images. It can be done by passing the argument <code class="highlighter-rouge">input_shape=(28, 28, 1)</code> to the first layer. Displaying the architecture of the convnet so far by calling <code class="highlighter-rouge">model.summary()</code>:</p> </blockquote> <blockquote> <p><strong>Output</strong>:</p> </blockquote> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>________________________________________________________________
Layer (type) Output Shape Param #
================================================================
conv2d_1 (Conv2D) (None, 26, 26, 32) 320
________________________________________________________________
maxpooling2d_1 (MaxPooling2D) (None, 13, 13, 32) 0
________________________________________________________________
conv2d_2 (Conv2D) (None, 11, 11, 64) 18496
________________________________________________________________
maxpooling2d_2 (MaxPooling2D) (None, 5, 5, 64) 0
________________________________________________________________
conv2d_3 (Conv2D) (None, 3, 3, 64) 36928
================================================================
Total params: 55,744
Trainable params: 55,744
Non-trainable params: 0
</code></pre></div></div> <blockquote> <p>The output of every <code class="highlighter-rouge">Conv2D</code> and <code class="highlighter-rouge">MaxPooling2D</code> layer is a <code class="highlighter-rouge">3D tensor</code> of shape<code class="highlighter-rouge"> (height, width, channels)</code>. The width and height dimensions tend to shrink as going deeper in the network. The number of channels is controlled by the first argument passed to the <code class="highlighter-rouge">Conv2D</code> layers (32 or 64).</p> </blockquote> <blockquote> <p>The next step is to feed the last output tensor (of shape <code class="highlighter-rouge">(3, 3, 64)</code>) into a densely connected classifier network: a stack of <code class="highlighter-rouge">Dense layers</code>. These classifiers process vectors, which are <code class="highlighter-rouge">1D</code>, whereas the current output is a <code class="highlighter-rouge">3D tensor</code>. First is to <code class="highlighter-rouge">flatten</code> the <code class="highlighter-rouge">3D</code> outputs to <code class="highlighter-rouge">1D</code>, and then add a few Dense layers on top.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
</code></pre></div></div> <blockquote> <p>Doing a 10-way classification by using a final layer with 10 outputs and a softmax activation.Displaying the architecture of the convnet so far by calling <code class="highlighter-rouge">model.summary()</code>:</p> </blockquote> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Layer (type) Output Shape Param #
================================================================
conv2d_1 (Conv2D) (None, 26, 26, 32) 320
________________________________________________________________
maxpooling2d_1 (MaxPooling2D) (None, 13, 13, 32) 0
________________________________________________________________
conv2d_2 (Conv2D) (None, 11, 11, 64) 18496
________________________________________________________________
maxpooling2d_2 (MaxPooling2D) (None, 5, 5, 64) 0
________________________________________________________________
conv2d_3 (Conv2D) (None, 3, 3, 64) 36928
________________________________________________________________
flatten_1 (Flatten) (None, 576) 0
________________________________________________________________
dense_1 (Dense) (None, 64) 36928
________________________________________________________________
dense_2 (Dense) (None, 10) 650
================================================================
Total params: 93,322
Trainable params: 93,322
Non-trainable params: 0
</code></pre></div></div> <blockquote> <p>The <code class="highlighter-rouge">(3, 3, 64) </code>outputs are flattened into vectors of shape <code class="highlighter-rouge">(576,)</code> before going through two Dense layers. Next is to train the CNNs on the MNIST digits.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>

<span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">test_labels</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p>Evaluate the model on the test data:</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
</code></pre></div></div> <blockquote> <p>Output:</p> </blockquote> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.99080000000000001
</code></pre></div></div> <p align="center"> <img src="/static/assets/img/blog/cnnoverall.jpeg" width="70%" /><br /> </p> <hr /> <blockquote> <p><strong>End –Cheng Gu</strong></p> </blockquote> </div> <hr> <div class="row"> <div class="col-md-6"> <h5 style="display: inline;">Tags:</h5> <button class="btn btn-white btn-xs" type="button">Python</button> <button class="btn btn-white btn-xs" type="button">Ai</button> </div> <div class="col-md-6"> <div class="small text-right"> <div> </div> </div> </div> </div> <br> <div class="row"> <div class="col-lg-12"> <!-- donate --> <br> <!-- share --> <div class="a2a_kit a2a_kit_size_32 a2a_default_style"> <a class="a2a_dd" href="https://www.addtoany.com/share"></a> <a class="a2a_button_facebook"></a> <a class="a2a_button_twitter"></a> <a class="a2a_button_google_plus"></a> <a class="a2a_button_linkedin"></a> <a class="a2a_button_email"></a> <a class="a2a_button_wechat"></a> <a class="a2a_button_sina_weibo"></a> <a class="a2a_button_pocket"></a> </div> <script> var a2a_config = a2a_config || {}; a2a_config.color_main = "D7E5ED"; a2a_config.color_border = "AECADB"; a2a_config.color_link_text = "333333"; a2a_config.color_link_text_hover = "333333"; </script> <script async src="https://static.addtoany.com/menu/page.js"></script> <br> <!-- comment --> </div> </div> </div> </div> </div> </div> </div> <!-- Google analytics --> <!-- GrowingIO --> </body> </html>
